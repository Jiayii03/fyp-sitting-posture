{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sitting Posture Detection Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostureDataset(Dataset):\n",
    "    def __init__(self, data, scaler=None, label_encoder=None, from_csv=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - data: CSV file path or DataFrame containing the dataset.\n",
    "        - scaler: StandardScaler for feature normalization.\n",
    "        - label_encoder: LabelEncoder for encoding the labels (not required if already encoded).\n",
    "        - from_csv: Set to True if the input is a CSV path.\n",
    "        \"\"\"\n",
    "        # Load data\n",
    "        if from_csv:\n",
    "            data = pd.read_csv(data)\n",
    "        \n",
    "        # Ensure required columns are present\n",
    "        if 'class' not in data.columns:\n",
    "            raise ValueError(\"The input data must contain a 'class' column.\")\n",
    "        \n",
    "        # Extract features and labels\n",
    "        self.X = data.drop(columns=['class']).values  # Features\n",
    "        self.y = data['class'].values  # Labels (already encoded)\n",
    "\n",
    "        # Normalize features using StandardScaler\n",
    "        if scaler:\n",
    "            self.X = scaler.transform(self.X)\n",
    "        else:\n",
    "            self.scaler = StandardScaler()\n",
    "            self.X = self.scaler.fit_transform(self.X)\n",
    "        \n",
    "        self.X = torch.tensor(self.X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(self.y, dtype=torch.long)  # Ensure labels are integers\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "    def get_class_info(self):\n",
    "        \"\"\"Print class encoding and number of samples per class.\"\"\"\n",
    "        print(\"\\nNumber of Samples per Class:\")\n",
    "        class_counts = pd.Series(self.y.numpy()).value_counts()\n",
    "        for idx, count in class_counts.items():\n",
    "            print(f\"Class {idx}: {count} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. MLP Model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(MLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),  # Hidden layer 1\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),  # Hidden layer 2\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_classes)  # Output layer\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated Training Function\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct_train, total_train = 0, 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "        \n",
    "        train_acc = correct_train / total_train\n",
    "        \n",
    "        # Validation Phase\n",
    "        model.eval()\n",
    "        val_loss, correct_val, total_val = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total_val += labels.size(0)\n",
    "                correct_val += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_acc = correct_val / total_val\n",
    "        \n",
    "        # Print epoch stats\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss/len(train_loader):.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss/len(val_loader):.4f}, Val Acc: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset:\n",
      "\n",
      "Number of Samples per Class:\n",
      "Class 0: 196 samples\n",
      "Class 1: 192 samples\n",
      "Class 3: 183 samples\n",
      "Class 2: 180 samples\n",
      "\n",
      "------------------------------------\n",
      "\n",
      "Testing dataset:\n",
      "\n",
      "Number of Samples per Class:\n",
      "Class 1: 98 samples\n",
      "Class 0: 80 samples\n",
      "Class 2: 73 samples\n",
      "Class 3: 71 samples\n",
      "\n",
      "------------------------------------\n",
      "\n",
      "Epoch [1/50], Train Loss: 1.2976, Train Acc: 0.4394, Val Loss: 1.2146, Val Acc: 0.4969\n",
      "Epoch [2/50], Train Loss: 1.1250, Train Acc: 0.5060, Val Loss: 1.0971, Val Acc: 0.4783\n",
      "Epoch [3/50], Train Loss: 1.0379, Train Acc: 0.5406, Val Loss: 1.0247, Val Acc: 0.5776\n",
      "Epoch [4/50], Train Loss: 0.9635, Train Acc: 0.6245, Val Loss: 0.9715, Val Acc: 0.6118\n",
      "Epoch [5/50], Train Loss: 0.8992, Train Acc: 0.6578, Val Loss: 0.9025, Val Acc: 0.6211\n",
      "Epoch [6/50], Train Loss: 0.8143, Train Acc: 0.6897, Val Loss: 0.8354, Val Acc: 0.6366\n",
      "Epoch [7/50], Train Loss: 0.7505, Train Acc: 0.6964, Val Loss: 0.7814, Val Acc: 0.6460\n",
      "Epoch [8/50], Train Loss: 0.6806, Train Acc: 0.7310, Val Loss: 0.7419, Val Acc: 0.6739\n",
      "Epoch [9/50], Train Loss: 0.6439, Train Acc: 0.7443, Val Loss: 0.7055, Val Acc: 0.7174\n",
      "Epoch [10/50], Train Loss: 0.6044, Train Acc: 0.7670, Val Loss: 0.6974, Val Acc: 0.7112\n",
      "Epoch [11/50], Train Loss: 0.5602, Train Acc: 0.7856, Val Loss: 0.6908, Val Acc: 0.7143\n",
      "Epoch [12/50], Train Loss: 0.5354, Train Acc: 0.7843, Val Loss: 0.6721, Val Acc: 0.7360\n",
      "Epoch [13/50], Train Loss: 0.5169, Train Acc: 0.8056, Val Loss: 0.6351, Val Acc: 0.7422\n",
      "Epoch [14/50], Train Loss: 0.5005, Train Acc: 0.8069, Val Loss: 0.6266, Val Acc: 0.7609\n",
      "Epoch [15/50], Train Loss: 0.4811, Train Acc: 0.8096, Val Loss: 0.6465, Val Acc: 0.7360\n",
      "Epoch [16/50], Train Loss: 0.4717, Train Acc: 0.8242, Val Loss: 0.6295, Val Acc: 0.7453\n",
      "Epoch [17/50], Train Loss: 0.4590, Train Acc: 0.8202, Val Loss: 0.6393, Val Acc: 0.7453\n",
      "Epoch [18/50], Train Loss: 0.4477, Train Acc: 0.8362, Val Loss: 0.6126, Val Acc: 0.7578\n",
      "Epoch [19/50], Train Loss: 0.4225, Train Acc: 0.8415, Val Loss: 0.6359, Val Acc: 0.7516\n",
      "Epoch [20/50], Train Loss: 0.4084, Train Acc: 0.8402, Val Loss: 0.6730, Val Acc: 0.7391\n",
      "Epoch [21/50], Train Loss: 0.4175, Train Acc: 0.8442, Val Loss: 0.6275, Val Acc: 0.7671\n",
      "Epoch [22/50], Train Loss: 0.3929, Train Acc: 0.8602, Val Loss: 0.6392, Val Acc: 0.7453\n",
      "Epoch [23/50], Train Loss: 0.3888, Train Acc: 0.8655, Val Loss: 0.6424, Val Acc: 0.7640\n",
      "Epoch [24/50], Train Loss: 0.3836, Train Acc: 0.8549, Val Loss: 0.6342, Val Acc: 0.7702\n",
      "Epoch [25/50], Train Loss: 0.3727, Train Acc: 0.8615, Val Loss: 0.6403, Val Acc: 0.7547\n",
      "Epoch [26/50], Train Loss: 0.3743, Train Acc: 0.8589, Val Loss: 0.6537, Val Acc: 0.7516\n",
      "Epoch [27/50], Train Loss: 0.3524, Train Acc: 0.8668, Val Loss: 0.6382, Val Acc: 0.7733\n",
      "Epoch [28/50], Train Loss: 0.3431, Train Acc: 0.8682, Val Loss: 0.6504, Val Acc: 0.7671\n",
      "Epoch [29/50], Train Loss: 0.3490, Train Acc: 0.8748, Val Loss: 0.6406, Val Acc: 0.7640\n",
      "Epoch [30/50], Train Loss: 0.3364, Train Acc: 0.8748, Val Loss: 0.6793, Val Acc: 0.7578\n",
      "Epoch [31/50], Train Loss: 0.3336, Train Acc: 0.8735, Val Loss: 0.6941, Val Acc: 0.7578\n",
      "Epoch [32/50], Train Loss: 0.3155, Train Acc: 0.8815, Val Loss: 0.6630, Val Acc: 0.7609\n",
      "Epoch [33/50], Train Loss: 0.3205, Train Acc: 0.8895, Val Loss: 0.6857, Val Acc: 0.7609\n",
      "Epoch [34/50], Train Loss: 0.3249, Train Acc: 0.8775, Val Loss: 0.6952, Val Acc: 0.7516\n",
      "Epoch [35/50], Train Loss: 0.3078, Train Acc: 0.8948, Val Loss: 0.6938, Val Acc: 0.7702\n",
      "Epoch [36/50], Train Loss: 0.2988, Train Acc: 0.8961, Val Loss: 0.6703, Val Acc: 0.7578\n",
      "Epoch [37/50], Train Loss: 0.2955, Train Acc: 0.8961, Val Loss: 0.6716, Val Acc: 0.7702\n",
      "Epoch [38/50], Train Loss: 0.2881, Train Acc: 0.8961, Val Loss: 0.6834, Val Acc: 0.7640\n",
      "Epoch [39/50], Train Loss: 0.2880, Train Acc: 0.8868, Val Loss: 0.6908, Val Acc: 0.7671\n",
      "Epoch [40/50], Train Loss: 0.2789, Train Acc: 0.8988, Val Loss: 0.7089, Val Acc: 0.7609\n",
      "Epoch [41/50], Train Loss: 0.2718, Train Acc: 0.9055, Val Loss: 0.6952, Val Acc: 0.7640\n",
      "Epoch [42/50], Train Loss: 0.2729, Train Acc: 0.9041, Val Loss: 0.7029, Val Acc: 0.7578\n",
      "Epoch [43/50], Train Loss: 0.2573, Train Acc: 0.9095, Val Loss: 0.7540, Val Acc: 0.7484\n",
      "Epoch [44/50], Train Loss: 0.2633, Train Acc: 0.8988, Val Loss: 0.7090, Val Acc: 0.7671\n",
      "Epoch [45/50], Train Loss: 0.2467, Train Acc: 0.9134, Val Loss: 0.7485, Val Acc: 0.7702\n",
      "Epoch [46/50], Train Loss: 0.2555, Train Acc: 0.9095, Val Loss: 0.7214, Val Acc: 0.7671\n",
      "Epoch [47/50], Train Loss: 0.2512, Train Acc: 0.9095, Val Loss: 0.7767, Val Acc: 0.7516\n",
      "Epoch [48/50], Train Loss: 0.2484, Train Acc: 0.9148, Val Loss: 0.7453, Val Acc: 0.7547\n",
      "Epoch [49/50], Train Loss: 0.2518, Train Acc: 0.9041, Val Loss: 0.7454, Val Acc: 0.7578\n",
      "Epoch [50/50], Train Loss: 0.2339, Train Acc: 0.9068, Val Loss: 0.7566, Val Acc: 0.7671\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed(RANDOM_SEED)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 50\n",
    "NUM_CLASSES = 4  # Crossed legs, Proper, Slouching, Reclining\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and preprocess data\n",
    "    csv_path = \"../../datasets/vectors/augmented_xy_filtered_keypoints_vectors_mediapipe.csv\"\n",
    "    data = pd.read_csv(csv_path)  # Load the dataset\n",
    "    \n",
    "    # Encode class labels to integers using LabelEncoder\n",
    "    label_encoder = LabelEncoder()\n",
    "    data['class'] = label_encoder.fit_transform(data['class'])\n",
    "\n",
    "    # Split the data into training and test datasets\n",
    "    train_data, test_data = train_test_split(data, test_size=0.3, random_state=RANDOM_SEED)\n",
    "\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    train_features = train_data.drop(columns=['class']).values\n",
    "    scaler.fit(train_features)\n",
    "\n",
    "    # Create datasets and loaders\n",
    "    train_dataset = PostureDataset(train_data, scaler=scaler, label_encoder=None, from_csv=False)\n",
    "    print(\"Training dataset:\")\n",
    "    train_dataset.get_class_info()\n",
    "    print(\"\\n------------------------------------\\n\")\n",
    "    test_dataset = PostureDataset(test_data, scaler=scaler, label_encoder=None, from_csv=False)\n",
    "    print(\"Testing dataset:\")\n",
    "    test_dataset.get_class_info()\n",
    "    print(\"\\n------------------------------------\\n\")\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    # Model initialization\n",
    "    input_size = train_features.shape[1]\n",
    "    model = MLP(input_size, NUM_CLASSES)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    # Train the model and calculate test loss\n",
    "    train_model(model, train_loader, test_loader, criterion, optimizer, epochs=EPOCHS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
